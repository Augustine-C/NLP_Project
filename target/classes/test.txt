Self-driving cars are rapidly becoming a reality. In 2016, car manufacturer Tesla an- nounced that all of its current cars were being equipped with the hardware necessary for autonomous driving (The Tesla Team, 2016). Since then, Tesla has incrementally enabled autonomous and assisted driving features via software updates (The Tesla Team, 2019). Other manufacturers have since been following suit (see Mercer and Macaulay, 2019) and the use of partially self-driving cars, such as these, is expected to increase within the next 20 years.
A major argument supporting the development of self-driving cars is the expected reduction in the number of traffic accidents. Of the more than 300,000 traffic accidents resulting in injuries to people in Germany in 2017, close to 90% were caused by driver misconduct or error, such as ignoring right of way, inappropriate following distance or speed, overtaking faults and driving under the influence of alcohol (Statistisches Bundesamt, 2018, p. 49). Similar observations have been made in both the United Kingdom and the United States (Department for Transport, 2013; National Highway Traffic Safety Administration, 2008). These errors and misconduct can potentially be mitigated by the introduction of self-driving cars, which highlights their potential to improve public safety.
The introduction of self-driving cars, however, is not without problems. Despite the expected reduction of the total number of accidents, unexpected situations will occur at a regular rate due to the large amount of road traffic worldwide (World Health Organization, 2015). It seems unrealistic to assume that safe handling of all unexpected traffic situations can be guaranteed. These situations are often highly complex and require split-second decisions. For this reason, human drivers are not generally expected to be able to respond optimally. Self-driving car control systems, on the other hand, can potentially estimate the outcome of various options within milliseconds and take actions that factor in an extensive body of research, debate, and legislation (Lin, 2015). The actions taken in such situations have potentially harmful consequences for car occupants, other traffic 2 participants, and pedestrians. Therefore, it is important to carefully consider the ethics of how self-driving cars will be designed to make decisions, an issue that is the topic of current debate (see Nyholm, 2018a,b).
Comprehensive guidelines for ethical decision making for self-driving cars have been provided by the ethics commission of the Federal Ministry of Transport and Digital Infrastructure (2017). These guidelines speak out against a standardised procedure of decision making in dilemma situations (guideline 8). In cases of unavoidable accidents,
“any distinction based on personal features (age, gender, physical or mental constitution) is strictly prohibited” and “[those] parties involved in the generation of mobility risks must not sacrifice non-involved parties” (guideline 9). These guidelines greatly add to the discussion and can inform the development of decision making systems. However, it is far from obvious that a practical implementation of these guidelines would garner public consensus.
Empirical research concerning what is acceptable to the public should not be neglected. It can be useful for highlighting areas problematic for the integration of self-driving cars into public traffic. Empirical research in the area of ethical decision making for self-driving cars has primarily focused on human decision making as a basis. In a typical experiment, participants make decisions pertaining to hypothetical dilemma situations in which harm is unavoidable. Situations of this kind, also known as ‘trolley dilemmas’ (Thomson, 1985), involve two groups of individuals, one of which must be endangered to spare the other. Simplified scenarios like these, although unlikely to occur in reality, allow for the pinpointing of factors that influence people’s decisions.
Moral dilemma studies can be grouped broadly into two paradigms: those that investigate moral judgements (what people claim are the right actions) and those that investigate moral actions (what people actually do in a given situation). An analysis of more than 40 million judgements on vignettes describing hypothetical dilemma situations concluded that people generally prefer self-driving cars to endanger fewer lives, endanger animals over people and endanger older people over younger people (Awad et al., 2018). Other moral judgement studies include a simulation study by Wintersberger et al. (2017) and online studies by Bonnefon et al. (2016) and Li et al. (2016). Importantly, Bonnefon et al. (2016) found a discrepancy between what people deemed acceptable for self-driving cars to do in dilemma situations and their willingness to purchase cars that would act accordingly. Specifically, people considered it more morally acceptable for self-driving cars to endanger fewer lives, even at the expense of the occupants’ lives, but preferred to purchase cars that would protect occupants. Martin et al. (2017) suggested that this discrepancy may be resolved if people explicitly consider the situations from both the 3 perspectives of car occupants and pedestrians.
Studies of moral action have used virtual reality environments to determine how human drivers would act when faced with dilemma situations. In these studies, participants were put in the perspective of drivers and controlled the steering of virtual vehicles when facing such dilemma situations. Skulmowski et al. (2014) placed participants in the role of train drivers and found participants generally preferred to save the greater number of lives. Su ̈tfeld et al. (2017) found that the behavior of participants in the role of car drivers could be well described by a value-of-life model, such that people are valued more than animals and younger people are valued more than older. Bergmann et al. (2018) and Faulhaber et al. (2018) showed that car drivers also act in ways that endanger fewer lives, even at the expense of their own.
While the results of these moral judgement and moral action studies have been generally consistent, there are important distinctions between the approaches needing consideration before making strong conclusions. First, there is growing evidence of discrepancies between what people consider to be the right action in moral dilemmas and what they would actually do (e.g. Francis et al., 2016; Gold et al., 2015; Patil et al., 2014; Tassy et al., 2013; FeldmanHall et al., 2012). Second, what is generally considered ethical for human drivers may not be the same for self-driving cars. Third, the perspective from which the situations are presented may affect how they are evaluated.
We performed two studies to address these distinctions and investigate their effects. In both studies, we recorded judgements pertaining to virtual dilemma situations involving either self-driving cars or human drivers. We included the perspectives of car occupants, uninvolved observers and pedestrians, which to our knowledge, no previous studies have done. Study 1 employed virtual reality to investigate judgements of highly ambiguous dilemma situations, while Study 2 used simplified animations and varied aspects of the situations in a more fine-grained manner.