When talking about the question “Could a machine think” and arguing the claim that a computer program itself can never achieve intentionality, the author first prompts two concepts—“strong” AI and “week” AI. A “weak” AI is used as a powerful tool in the process of formulating and testing hypotheses. A “Strong” AI, however, is a mind with cognitive states and the capability of intentionality. The discussion here is mostly about the “string” AI.
The author argues that the programs that are said to be able to understand stories and provide answers to questions cannot be said to show understanding of the story nor to explain the human ability to understand and reasoning. The author then described a Chinese room situation where one non-Chinese speaker is in a room, and the person is given three batches of Chinese symbols with the first one being a script, the second being a story, and the third being answers. The person is then asked to use rules provided in the room to form responses using those Chinese symbols. The author argues that though it may seem to the outside world that the participant understands Chinese, the participant knows nothing about Chinese. The same can apply to the machines—they can demonstrate human-like behavior without an actual understanding of their input information.
The author further argues with the replies he got from others. The author believes that the two systems (one English and the other Chinese) in the participant are not remotely alike. For the first English system, the participant knows the reference of food in the story, whereas, for the second system, the participant knows nothing but the symbols and the rules of transforming the input to output.